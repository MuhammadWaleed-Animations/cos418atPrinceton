package main

import (
	"fmt"
	"os"
	"src/mapreduce"

	"unicode"

	"sort"
	"strconv"
	"strings"
)

// The mapping function is called once for each piece of the input.
// In this framework, the key is the name of the file that is being processed,
// and the value is the file's contents. The return value should be a slice of
// key/value pairs, each represented by a mapreduce.KeyValue.
// mapF extracts words and emits (word, document) pairs
// func mapF(document string, value string) (res []mapreduce.KeyValue) {
// 	isLetter := func(c rune) bool { return unicode.IsLetter(c) }
// 	words := strings.FieldsFunc(value, func(c rune) bool { return !isLetter(c) })

// 	wordSet := make(map[string]bool) // To avoid duplicate (word, document) pairs

// 	for _, word := range words {
// 		word = strings.ToLower(word) // Normalize case
// 		if !wordSet[word] {
// 			wordSet[word] = true
// 			res = append(res, mapreduce.KeyValue{Key: word, Value: document})
// 		}
// 	}

// 	return res
// }

// mapF extracts words and emits (word, document) pairs
func mapF(document string, value string) (res []mapreduce.KeyValue) {
	// Function to check if a rune is a letter
	isLetter := func(c rune) bool { return unicode.IsLetter(c) }

	// Extract words using FieldsFunc
	words := strings.FieldsFunc(value, func(c rune) bool { return !isLetter(c) })

	// Use a set to avoid duplicate (word, document) pairs
	wordSet := make(map[string]bool)

	for _, word := range words {
		word = strings.ToLower(strings.TrimSpace(word)) // Normalize case and trim spaces

		// ðŸ”¥ Ensure only valid words are processed
		if word == "" || len(word) < 2 {
			continue // Skip empty strings and single-character words
		}
		if !wordSet[word] {
			wordSet[word] = true
			res = append(res, mapreduce.KeyValue{Key: word, Value: document})
		}
	}

	return res
}

// The reduce function is called once for each key generated by Map, with a
// list of that key's string value (merged across all inputs). The return value
// should be a single output value for that key.

// reduceF aggregates documents and returns a formatted string
// func reduceF(key string, values []string) string {
// 	docSet := make(map[string]bool) // Unique documents

// 	for _, doc := range values {
// 		docSet[doc] = true
// 	}

// 	// Convert map keys to a sorted slice
// 	var docs []string
// 	for doc := range docSet {
// 		docs = append(docs, doc)
// 	}
// 	sort.Strings(docs)

// 	// Return formatted output: "count doc1,doc2,..."
// 	return strconv.Itoa(len(docs)) + " " + strings.Join(docs, ",")
// }

// reduceF aggregates document occurrences and formats output correctly
func reduceF(key string, values []string) string {
	docSet := make(map[string]bool)

	// Ensure unique document entries
	for _, doc := range values {
		docSet[doc] = true
	}

	// Convert map keys to a sorted slice
	var docs []string
	for doc := range docSet {
		docs = append(docs, doc)
	}
	sort.Strings(docs) // Sort documents

	// ðŸ”¥ Trim spaces and ensure formatting consistency
	return strconv.Itoa(len(docs)) + " " + strings.Join(docs, ",")
}

// Can be run in 3 ways:
// 1) Sequential (e.g., go run wc.go master sequential x1.txt .. xN.txt)
// 2) Master (e.g., go run wc.go master localhost:7777 x1.txt .. xN.txt)
// 3) Worker (e.g., go run wc.go worker localhost:7777 localhost:7778 &)
func main() {
	if len(os.Args) < 4 {
		fmt.Printf("%s: see usage comments in file\n", os.Args[0])
	} else if os.Args[1] == "master" {
		var mr *mapreduce.Master
		if os.Args[2] == "sequential" {
			mr = mapreduce.Sequential("iiseq", os.Args[3:], 3, mapF, reduceF)
		} else {
			mr = mapreduce.Distributed("iiseq", os.Args[3:], 3, os.Args[2])
		}
		mr.Wait()
	} else {
		mapreduce.RunWorker(os.Args[2], os.Args[3], mapF, reduceF, 100)
	}
}
